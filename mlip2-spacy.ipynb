{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "0. add levenshtein distance to naive model to get best predictions\n",
    "0. during the submit process, save all new dataset names in the naive model.\n",
    "0. after NLP prediction, use levenshtein distance to find previously known dataset. If distance is small enough, use the known dataset\n",
    "1. do data pre-processing (cleaning etc) IMPORTANT: cleaning the whole text may result in inconsistencies between the dataset name in our cleaned text, and the dataset label in the answers csv\n",
    "3. add naive baseline (with a buffer of all known datasets) and combine with spacy model\n",
    "4. add SciBERT as a third model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\n",
      "Cuda compilation tools, release 11.2, V11.2.67\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.0.6-cp37-cp37m-win_amd64.whl (11.7 MB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (2.18.4)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.3-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (20.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (2.11.1)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp37-cp37m-win_amd64.whl (450 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (1.16.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading spacy_legacy-3.0.5-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in c:\\users\\31640\\anaconda3\\lib\\site-packages (from spacy) (3.7.4.3)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-win_amd64.whl (6.5 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading catalogue-2.0.4-py3-none-any.whl (16 kB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\31640\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\31640\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in c:\\users\\31640\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.3->spacy) (2.2.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107102 sha256=b797b800390af18fdc2dfd6087fde5cd5484284e56ffe7774ede6388f999380c\n",
      "  Stored in directory: c:\\users\\31640\\appdata\\local\\pip\\cache\\wheels\\83\\a6\\12\\bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: wasabi, pydantic, click, typer, murmurhash, cymem, preshed, catalogue, srsly, blis, thinc, smart-open, pathy, spacy-legacy, spacy\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.4 click-7.1.2 cymem-2.0.5 murmurhash-1.0.5 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b1c95e226c4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# !IMPORTANT: turn this off when gpu not turned on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.require_gpu() # !IMPORTANT: turn this off when gpu not turned on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input contains 0 files\n",
      "/kaggle/input/coleridgeinitiative-show-us-the-data contains 2 files\n",
      "/kaggle/input/coleridgeinitiative-show-us-the-data/test contains 4 files\n",
      "/kaggle/input/coleridgeinitiative-show-us-the-data/train contains 14316 files\n",
      "/kaggle/input/mlip2-spacy contains 6 files\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "from math import floor\n",
    "from random import shuffle\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(f\"{dirname} contains {len(filenames)} files\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filepath = \"../input/mlip2-spacy/2021-05-04 21:11:55.931579.sav\" # SET TO 'None' if you want to train from scratch!\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Converts all text to lower case, Removes special characters, emojis and multiple spaces\n",
    "    text - Sentence that needs to be cleaned\n",
    "    '''\n",
    "    text = ''.join([k for k in text if k not in string.punctuation])\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    training_data = []\n",
    "    \n",
    "    # open the csv with id's, data labels, etc. and append the json files to it\n",
    "    files = []\n",
    "    train_dir = '../input/coleridgeinitiative-show-us-the-data/train' # location of the training json files\n",
    "    df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv') # location of the training csv file (does not contain the actual texts)\n",
    "    for i in df.index:\n",
    "        file_id = df['Id'][i]\n",
    "        filename = f\"{file_id}.json\"\n",
    "        filepath = os.path.join(train_dir, filename)\n",
    "        with open(filepath) as json_file:\n",
    "            file = json.loads(json_file.read())\n",
    "            files.append(file)\n",
    "    df['file'] = files\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df = load_train_data()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This study used data from the National Education Longitudinal Study (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment. The study also reported whether the impacts of dual enrollment programs were different for first generation college students versus students whose parents had attended at least some college. In addition, a supplemental analysis reports on the impact of different amounts of dual enrollment course-taking and college degree attainment.\\nDual enrollment programs offer college-level learning experiences for high school students. The programs offer college courses and/or the opportunity to earn college credits for students while still in high school.\\nThe intervention group in the study was comprised of NELS participants who attended a postsecondary school and who participated in a dual enrollment program while in high school (n = 880). The study author used propensity score matching methods to create a comparison group of NELS participants who also attended a postsecondary school but who did not participate in a dual enrollment program in high school (n = 7,920).', {'entities': [(30, 67, 'DATASET')]})\n"
     ]
    }
   ],
   "source": [
    "# some config variables\n",
    "MIN_LENGTH_SAMPLE = 10\n",
    "MAX_LENGTH_SAMPLE = 9000\n",
    "\n",
    "def format_dataframe_for_spacy(df):\n",
    "    data = []\n",
    "    for i in df.index:\n",
    "        # dataset name, to be used as training label\n",
    "        file = df['file'][i]\n",
    "        dataset_title = df['dataset_title'][i]\n",
    "\n",
    "        for section in file:\n",
    "            # each section contains a 'section_title' and a 'text' key, for now we only use 'text'\n",
    "            text = section['text']\n",
    "\n",
    "            # only consider this sample when these rules apply\n",
    "            if len(text) < MIN_LENGTH_SAMPLE:\n",
    "                continue\n",
    "            if len(text) > MAX_LENGTH_SAMPLE:\n",
    "                continue\n",
    "            # END OF RULES\n",
    "\n",
    "            # !IMPORTANT TODO: Adding padding to the dataset title removes about 1/3rd of the training data. probably not good\n",
    "            if f' {dataset_title} ' in text: # Only use a section as a training sample IF it contains a dataset label\n",
    "                start_index = text.find(dataset_title)\n",
    "                end_index = start_index + len(dataset_title)\n",
    "                entity = (start_index, end_index, 'DATASET')\n",
    "                entities = [entity]\n",
    "                sample = (text, {'entities': entities})\n",
    "                data.append(sample)\n",
    "                # TODO: What if it finds two matches in a file?\n",
    "                        \n",
    "    return data\n",
    "\n",
    "spacy_data = format_dataframe_for_spacy(df)\n",
    "print(spacy_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60', '2100032a-7c33-4bff-97ef-690822c43466', '2f392438-e215-4169-bebf-21ac4ff253e1', '3f316b38-1a24-45a9-8d8c-4e05a42257c6'])\n"
     ]
    }
   ],
   "source": [
    "def load_test_data():\n",
    "    test_data = {}\n",
    "    for dirname, _, filenames in os.walk('../input/coleridgeinitiative-show-us-the-data/test'):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "            with open(filepath) as json_file:\n",
    "                file = json.loads(json_file.read())\n",
    "                file_id = filename.replace(\".json\", \"\")\n",
    "                test_data[file_id] = file\n",
    "    return test_data\n",
    "\n",
    "TEST_DATA = load_test_data()\n",
    "print(TEST_DATA.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = floor(len(spacy_data) * 0.9)\n",
    "# TRAIN_DATA = spacy_data[:train_len]\n",
    "# VAL_DATA = spacy_data[train_len:]\n",
    "# print(len(TRAIN_DATA))\n",
    "# print(len(VAL_DATA))\n",
    "\n",
    "# TRAIN_DATA = spacy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blank_nlp(train_data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp = spacy.blank(\"en\") # TODO: Either create a blank one and add a pipe, or download one and replace the NER pipe...\n",
    "    nlp.remove_pipe(\"ner\") # If creating from blank, this line of code should be commented\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def load_nlp_from_file(filename):\n",
    "    with open(filename, 'rb') as pickle_file:\n",
    "        model = pickle.load(pickle_file)\n",
    "        return model\n",
    "    \n",
    "def save_nlp_to_file(model):\n",
    "    filename = f\"{dt.datetime.now()}.sav\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.load(name, **overrides)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip show spacy\n",
    "spacy.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f9cc0dbc7d0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f9cc0a450c0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f9cc09dd7c0>)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_nlp_to_file(nlp)\n",
    "nlp = load_nlp_from_file(\"./bytes_data.sav\")\n",
    "text = \"The spur of ICT (Information and Communication Technologies) innovations in the twenty-first century has massively disrupted economies and business models (Christensen, 2013; Tohmatsu, 2012) . Millions of jobs face a high probability of being replaced because computers and the internet are reshaping the labor market (Oliver, 2015) . In the framework of globalization, digital skills are now considered preliminary for securing professional employment across the globe (Pirzada & Khan, 2013) . Consequently, many employers across a wide range of sectors are increasingly viewing ICT skills as an important component of employability (Belt & Richardson, 2005; Johnson & Burden, 2003) . The omnipresence of computing has made digital literacy increasingly critical to success in any occupation (Murray & P\\u00e9rez, 2014) . In fact, organizations are now identifying digital skills or computer literacy as one of their core values for employability (such as the US Department of Education, the US Department of commerce, the OECD Program for the International Assessment of Adult Competencies and the European Commission).\\nIn particular, in developing countries, digital skills are estimated to reduce poverty levels and increase employment rates (Bennett, Maton, & Kervin, 2008) . ICT competence can bring significant benefits to marginalized groups, allowing these groups to participate more fully in society as it improves employment opportunities, overcomes isolation, builds confidence and leads to further learning (Bunker, 2010) . It\\\"s not a surprise that Digital Literacy has been deemed an essential life skill (Europea, 2008) . Individuals\\\" digital engagement has direct implications on their academic performance, labor market success and entrepreneurship uptakes. Those who function better in the digital realm and participate more fully in digitally mediated life enjoy advantages over their digitally disadvantaged counterparts (Robinson et al., 2015) . ICT have become central to every economy and to people\\\"s quality of life in every society (Olatoye, 2011) .\\nNevertheless, a new form of inequality is emerging, adding to all the existing forms of discrimination (Hilbert, 2011) . Concerns are being raised that the digital divide is leaving behind those most in need of assistance (Greig, 2004) . It is feared that as the role of ICT continues to expand, the exclusion experienced by disadvantaged groups may be accentuated and reinforced, rather than mitigated (Corrigan & Joyce, 2000) . Discussions about digital divides typically refer to socio-economic disparities in accessibility and usage of ICT. The use of such technologies result in several beneficial outcomes while the non-use of such technologies excludes people from full participation in contemporary society (Helsper, Deursen, & Eynon, 2015) . Many studies have established that basic access to digital resources and the skills to effectively use them still escape many economically disadvantaged or traditionally underrepresented portions of the population (Witte & Mannon, 2010) . Thus, digital inequalities can reinforce and exacerbate existing social inequalities (DiMaggio & Garip, 2012) . Further interventions are needed to ensure that the unqualified, the low skilled, the long-term unemployed and those on low incomes are enabled to reap the benefits of new services and opportunities for job seeking through ICT (Lindsay, 2005) .\"\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../input/mlip2-spacy/2021-05-04 21:11:55.931579.sav...\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from tqdm import trange\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "def train_nlp_model():\n",
    "    nlp = create_blank_nlp(TRAIN_DATA)\n",
    "    optimizer = nlp.begin_training()\n",
    "    t = trange(EPOCHS, desc='After first epoch, this will show losses', leave=True)\n",
    "    for i in t:\n",
    "        losses = {}\n",
    "        batch_gen = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        batches = list(batch_gen)\n",
    "        batch_counter = 0\n",
    "        for batch in batches:\n",
    "            t.set_description(f'epoch: {i} | batch {batch_counter} of {len(batches)}')\n",
    "            batch_counter += 1\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.1,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        t.write(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")\n",
    "    return nlp\n",
    "\n",
    "    \n",
    "nlp = None\n",
    "if saved_model_filepath:\n",
    "    print(f\"Loading model from: {saved_model_filepath}...\")\n",
    "    nlp = load_nlp_from_file(saved_model_filepath)\n",
    "    print(\"Loaded model\")\n",
    "else:\n",
    "    print(\"Training model from scratch...\")\n",
    "    nlp = train_nlp_model()\n",
    "    print(\"Model trained\")\n",
    "    if save_model:\n",
    "        print(\"Saving trained model...\")\n",
    "        save_nlp_to_file(nlp)\n",
    "        print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive model\n",
    "keeps track of all dataset labels in our training data, and checks if a sample contains one of these, and if so, returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel():\n",
    "    def __init__(self, df):\n",
    "        self.labels = []\n",
    "        self.df = df\n",
    "        \n",
    "    def train(self):\n",
    "        for i in self.df.index:\n",
    "            label = self.df[\"dataset_title\"][i]\n",
    "            if not label in self.labels:\n",
    "                self.labels.append(label)\n",
    "            \n",
    "        \n",
    "    def predict(self, file):\n",
    "        for section in file:\n",
    "            for label in self.labels:\n",
    "                  if label.lower() in section['text'].lower():\n",
    "                    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create naive model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = NaiveModel(df)\n",
    "naive.train()\n",
    "print(f'Naive model has collected: {len(naive.labels)} unique labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility class to get insight into some meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def increment_naive(self):\n",
    "        self.naive += 1\n",
    "        \n",
    "    def increment_nlp(self):\n",
    "        self.nlp += 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.naive = 0\n",
    "        self.nlp = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict function that takes in some models, and tries to predict a dataset title for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(naive, nlp, file, counter):\n",
    "    # naive model first\n",
    "    label = naive.predict(file)\n",
    "    if label:\n",
    "        counter.increment_naive()\n",
    "        return label\n",
    "    \n",
    "    return \"NOT FOUND\" # !IMPORTANT TODO: REMOVE THIS\n",
    "    \n",
    "    # if naive model cannot find it, use spacy model\n",
    "    for section in file:\n",
    "        doc = nlp(section['text'])\n",
    "        for ent in doc.ents:\n",
    "            # TODO: this ONLY CONSIDERS THE FIRST ONE. This first guess might be wrong, and the actual filename might be found further in the doc\n",
    "            if ent.label_ == \"DATASET\":\n",
    "                counter.increment_nlp()\n",
    "                return ent.text\n",
    "            \n",
    "    # If none of the models find anything\n",
    "    return \"NOT FOUND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "\n",
    "# temp code for checking stuff / debugging\n",
    "# temp_test_set = {}\n",
    "# for i in df.index:\n",
    "#     if (len(temp_test_set.items()) > 100):\n",
    "#         continue\n",
    "#     key = df[\"Id\"][i]\n",
    "#     file = df[\"file\"][i]\n",
    "#     temp_test_set[key] = file\n",
    "\n",
    "def submit(data):\n",
    "    buffer = {}\n",
    "    for file_id, file in data.items():\n",
    "        dataset_label = predict_one(naive, nlp, file, counter)\n",
    "        buffer[file_id] = dataset_label\n",
    "    \n",
    "    ids = buffer.keys()\n",
    "    predictions = buffer.values()\n",
    "    data = {\"Id\": ids, \"PredictionString\": predictions }\n",
    "    df = pd.DataFrame(data=data)\n",
    "    df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # print some meta data\n",
    "    print(f'naive predicted: {counter.naive}')\n",
    "    print(f'nlp predicted: {counter.nlp}')\n",
    "\n",
    "# submit(temp_test_set)\n",
    "submit(TEST_DATA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
